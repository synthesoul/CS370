{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# MNIST Neural Network - CS 370 Module 2\n", "## Name: Christian Busca\n", "## Date: March 16, 2025\n", "\n", "This notebook loads the MNIST dataset, trains a neural network with two hidden layers, and evaluates accuracy. Additionally, we modify parameters to analyze their impact.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Load and execute the external Python script\n", "%run mnist_model.py"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## **Observations and Conclusion**\n", "\n", "### **Experiment 1: Increasing Hidden Neurons (128 \u2192 256)**\n", "- **Training Accuracy:** Increased significantly.\n", "- **Validation Accuracy:** Improved slightly from ~94.63% to ~95.07%.\n", "- **Test Accuracy:** Small improvement from 94.63% \u2192 **94.76%**.\n", "\n", "\ud83d\udccc *Analysis:* Increasing neurons slightly improved accuracy, but gains were minimal. This suggests that adding more neurons captures complex patterns but has diminishing returns.\n", "\n", "---\n", "### **Experiment 2: Changing Batch Size (128 \u2192 64)**\n", "- **Training Accuracy:** Slightly lower than previous runs.\n", "- **Validation Accuracy:** Improved slightly from 95.07% to **96.42%**.\n", "- **Test Accuracy:** Improved from 94.76% \u2192 **96.29%**.\n", "\n", "\ud83d\udccc *Analysis:* Reducing the batch size allowed more frequent updates, improving generalization. However, it slightly increased training time.\n", "\n", "---\n", "### **Experiment 3: Changing Optimizer to Adam (from SGD)**\n", "- **Training Accuracy:** Improved significantly compared to SGD.\n", "- **Validation Accuracy:** Increased further to **96.42%**.\n", "- **Test Accuracy:** Improved to **96.29%**.\n", "\n", "\ud83d\udccc *Analysis:* The Adam optimizer outperformed SGD in both training speed and accuracy. It led to faster convergence and better generalization.\n", "\n", "---\n", "### **Final Conclusion**\n", "- **Increasing neurons had a small effect on accuracy**, but overfitting risks remain.\n", "- **Reducing batch size improved generalization**, though training took longer.\n", "- **Switching to Adam optimizer had the biggest impact**, improving test accuracy to 96.29%.\n", "- **Future improvements** could include using Convolutional Neural Networks (CNNs) instead of dense layers or applying regularization techniques (e.g., dropout) to further reduce overfitting.\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8.10"}}, "nbformat": 4, "nbformat_minor": 4}